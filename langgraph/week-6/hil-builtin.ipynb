{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6e8b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential,get_bearer_token_provider\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing  import Literal,Sequence,Annotated,TypedDict\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")  # Update to your API key\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_APIKEY\")\n",
    "\n",
    "\n",
    "token_provider= get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "model= AzureChatOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://azopenai-langchain.openai.azure.com/\",\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    azure_deployment= \"gpt-4o-mini\"\n",
    ")\n",
    "model.invoke(\"HI\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe0c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a:int,b:int):\n",
    "    \"\"\" This tool will multiply the given two imput and return the response\"\"\"\n",
    "    return a*b\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query:str):\n",
    "    \"\"\"search the web for a query and return the respone\"\"\"\n",
    "    web_results= TavilySearchResults(max_results=1).invoke(query)\n",
    "    return f\"Result for {query} is ::  {web_results}\"\n",
    "tools = [multiply,search]\n",
    "llm_with_tools = model.bind_tools(tools=tools)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence[BaseMessage],operator.add]\n",
    "\n",
    "def ai_assistant(state: AgentState):\n",
    "   print(\"in assitant\")\n",
    "   response=  llm_with_tools.invoke(state[\"messages\"])\n",
    "   print(response)\n",
    "   return {\"messages\":[response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedfa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory= MemorySaver()\n",
    "graph= StateGraph(AgentState)\n",
    "graph.add_node(\"ai_assistant\",ai_assistant)\n",
    "graph.add_node(\"tools\",ToolNode([multiply,search]))\n",
    "graph.add_conditional_edges(\"ai_assistant\",tools_condition)\n",
    "graph.add_edge(\"tools\",\"ai_assistant\")\n",
    "graph.add_edge(START, \"ai_assistant\")\n",
    "config= {\"configurable\" : {\"thread_id\" :\"1\"}}\n",
    "compiledgraph=graph.compile(checkpointer=memory,interrupt_before=[\"tools\"])\n",
    "compiledgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf155f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage,ToolMessage\n",
    "new_message=[\n",
    "    ToolMessage\n",
    "]\n",
    "response=compiledgraph.invoke({\"messages\":[HumanMessage(\"What is the current gdp of the china?\")]},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot=compiledgraph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot.next\n",
    "tool_details=snapshot.values[\"messages\"][-1].tool_calls\n",
    "tool_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a33cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tool_details[0][\"name\"]== \"search\":\n",
    "    user_input=input(prompt=f\"[yes/no] do you want to continue with {tool_details[0]['name']}?\").lower()\n",
    "    if user_input==\"no\":\n",
    "        print(\"web tool discarded\")\n",
    "        raise Exception(\"Web tool discarded by the user.\")\n",
    "    else:\n",
    "        response=compiledgraph.invoke(None,config)\n",
    "        print(response)\n",
    "else:\n",
    "    response=compiledgraph.invoke(None,config)\n",
    "    print(response)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ef3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiledgraph.get_state(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
