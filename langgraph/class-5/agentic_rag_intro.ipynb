{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b250c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os,bs4\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential,get_bearer_token_provider\n",
    "from langchain_openai import AzureChatOpenAI,AzureOpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from typing import  TypedDict,Literal,Annotated,Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "from langchain import hub\n",
    "from pydantic import BaseModel,Field\n",
    "from langgraph.graph import StateGraph,START,END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6c1c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "az_chat_model= AzureChatOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://azopenai-langchain.openai.azure.com/\",\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    model= \"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "embedding_model= AzureOpenAIEmbeddings(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://azopenai-langchain.openai.azure.com/\",\n",
    "    azure_ad_token_provider= token_provider,\n",
    "    model= \"text-embedding-ada-002\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd08079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_query(\"hai, hello how are you\"))\n",
    "#1536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72769b50",
   "metadata": {},
   "source": [
    "- LangGraph\n",
    "- Crew AI\n",
    "- Agno\n",
    "\n",
    "###### Explore What is Agentic RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ddf5d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content=\"Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\nPrompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\\n\\nText: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.\\nSentiment: negative\\n\\nText: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.\\nSentiment: positive\\n\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nMany studies looked into how to construct in-context examples to maximize the performance and observed that choice of prompt format, training examples, and the order of the examples can lead to dramatically different performance, from near random guess to near SoTA.\\nZhao et al. (2021) investigated the case of few-shot classification and proposed that several biases with LLM (they use GPT-3 in the experiments) contribute to such high variance: (1) Majority label bias exists if distribution of labels among the examples is unbalanced; (2) Recency bias refers to the tendency where the model may repeat the label at the end; (3) Common token bias indicates that LLM tends to produce common tokens more often than rare tokens. To conquer such bias, they proposed a method to calibrate the label probabilities output by the model to be uniform when the input string is N/A.\\nTips for Example Selection#\\n\\n\\nChoose examples that are semantically similar to the test example using $k$-NN clustering in the embedding space (Liu et al., 2021)\"),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='For example to produce education materials for kids,\\n\\nDescribe what is quantum physics to a 6-year-old.\\n\\nAnd safe content,\\n\\n... in language that is safe for work.\\nIn-context instruction learning (Ye et al. 2023) combines few-shot learning with instruction prompting. It incorporates multiple demonstration examples across different tasks in the prompt, each demonstration consisting of instruction, task input and output. Note that their experiments were only on classification tasks and the instruction prompt contains all label options.\\nDefinition: Determine the speaker of the dialogue, \"agent\" or \"customer\".\\nInput: I have successfully booked your tickets.\\nOuput: agent\\n\\nDefinition: Determine which category the question asks for, \"Quantity\" or \"Location\".\\nInput: What\\'s the oldest building in US?\\nOuput: Location\\n\\nDefinition: Classify the sentiment of the given movie review, \"positive\" or \"negative\".\\nInput: i\\'ll bet the video game is a lot more fun than the film.\\nOutput:\\nSelf-Consistency Sampling#\\nSelf-consistency sampling (Wang et al. 2022a) is to sample multiple outputs with temperature > 0 and then selecting the best one out of these candidates.\\nThe criteria for selecting the best candidate can vary from task to task. A general solution is to pick majority vote. For tasks that are easy to validate such as a programming question with unit tests, we can simply run through the interpreter and verify the correctness with unit tests.\\nChain-of-Thought (CoT)#\\nChain-of-thought (CoT) prompting (Wei et al. 2022) generates a sequence of short sentences to describe reasoning logics step by step, known as reasoning chains or rationales, to eventually lead to the final answer. The benefit of CoT is more pronounced for complicated reasoning tasks, while using large models (e.g. with more than 50B parameters). Simple tasks only benefit slightly from CoT prompting.\\nTypes of CoT prompts#\\nTwo main types of CoT prompting:\\n\\nFew-shot CoT. It is to prompt the model with a few demonstrations, each containing manually written (or model-generated) high-quality reasoning chains.\\n\\n(All the math reasoning examples are from GSM8k)\\nQuestion: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?\\nAnswer: It takes Tom 30*4 = <<30*4=120>>120 minutes to climb the hill.\\nIt takes Tom 120/60 = <<120/60=2>>2 hours to climb the hill.\\nSo the answer is 2.\\n===\\nQuestion: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost $9.50, and the shoes cost $92. Jack has $40. How much more money does Jack need?\\nAnswer: The total cost of two pairs of socks is $9.50 x 2 = $<<9.5*2=19>>19.\\nThe total cost of the socks and the shoes is $19 + $92 = $<<19+92=111>>111.\\nJack need $111 - $40 = $<<111-40=71>>71 more.\\nSo the answer is 71.\\n===\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer:\\n\\nZero-shot CoT. Use natural language statement like Let\\'s think step by step to explicitly encourage the model to first generate reasoning chains and then to prompt with Therefore, the answer is to produce answers (Kojima et al. 2022 ). Or a similar statement Let\\'s work this out it a step by step to be sure we have the right answer (Zhou et al. 2022).\\n\\nQuestion: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\\nAnswer: Let\\'s think step by step.\\nTips and Extensions#\\n\\n\\nSelf-consistency sampling can improve reasoning accuracy by sampling a number of diverse answers and then taking the majority vote. (Wang et al. 2022a)'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'}, page_content='References#\\n[1] Zhao et al. “Calibrate Before Use: Improving Few-shot Performance of Language Models.” ICML 2021\\n[2] Liu et al. “What Makes Good In-Context Examples for GPT-3?” arXiv preprint arXiv:2101.06804 (2021).\\n[3] Lu et al. “Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.” ACL 2022\\n[4] Ye et al. “In-Context Instruction Learning.” arXiv preprint arXiv:2302.14691 (2023).\\n[5] Su et al. “Selective annotation makes language models better few-shot learners.” arXiv preprint arXiv:2209.01975 (2022).\\n[6] Rubin et al. “Learning to retrieve prompts for in-context learning.” NAACL-HLT 2022\\n[7] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).\\n[13] Press et al. “Measuring and narrowing the compositionality gap in language models.” arXiv preprint arXiv:2210.03350 (2022).\\n[14] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[15] Fu et al. “Complexity-based prompting for multi-step reasoning.” arXiv preprint arXiv:2210.00720 (2022).\\n[16] Wang et al. “Rationale-augmented ensembles in language models.” arXiv preprint arXiv:2207.00747 (2022).\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n[18] Shum et al. “Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.” arXiv preprint arXiv:2302.12822 (2023).\\n[19] Zhou et al. “Large Language Models Are Human-Level Prompt Engineers.” ICLR 2023.\\n[20] Lazaridou et al. “Internet augmented language models through few-shot prompting for open-domain question answering.” arXiv preprint arXiv:2203.05115 (2022).\\n[21] Chen et al. “Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.” arXiv preprint arXiv:2211.12588 (2022).\\n[22] Gao et al. “PAL: Program-aided language models.” arXiv preprint arXiv:2211.10435 (2022).\\n[23] Parisi et al. “TALM: Tool Augmented Language Models” arXiv preprint arXiv:2205.12255 (2022).\\n[24] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Prompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)\\n\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading\n",
    "loader= WebBaseLoader(web_paths=[\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "                        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\n",
    "                      ],\n",
    "                      bs_kwargs=dict(\n",
    "                            parse_only= bs4.SoupStrainer(\n",
    "                                class_ = (\"post-title\",\"post-content\",\"post-header\")\n",
    "                            )\n",
    "                      )\n",
    "                      )\n",
    "documents= loader.load()\n",
    "\n",
    "# Splitting\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size =1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embedding and Storing\n",
    "vector_db=Chroma.from_documents(\n",
    "    collection_name= \"rag-chroma\",\n",
    "    embedding= embedding_model,\n",
    "    documents= chunks\n",
    ")\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "retriever.invoke(\"what is agent and prompt?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e210638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever=retriever,name = \"Blog-retriver\",description= \"This will fetch blog documents from Chroma DB\")\n",
    "\n",
    "tools =[retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "133b500e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a16c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relavance score 'yes' or 'no'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "798f1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "def llm_decision_maker(state : AgentState) ->AgentState:\n",
    "    print(\"LLM Decision Maker Call.....\")\n",
    "    prompt=PromptTemplate(\n",
    "    template=\"\"\"You are a helpful assistant whatever question has been asked to find out that in the given question and answer.\n",
    "                    Here is the question:{question}\n",
    "                    \"\"\",\n",
    "                    input_variables=[\"question\"]\n",
    "                    )\n",
    "    messages = state[\"messages\"]\n",
    "    if len(messages)>1:\n",
    "        chain = prompt | az_chat_model\n",
    "        response =  chain.invoke({\"question\":messages[0]})\n",
    "        return {\"messages\": [response.content]}\n",
    "    else:\n",
    "         llm_with_tool= az_chat_model.bind_tools(tools=tools)\n",
    "         response = llm_with_tool.invoke(messages)   \n",
    "         return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def grade_documents(state: AgentState)  ->Literal[\"generator\",\"rewriter\"] :\n",
    "    print(\"Grade Called.....\")\n",
    "    az_structured_model = az_chat_model.with_structured_output(grade)\n",
    "    messages = state[\"messages\"]\n",
    "    lastmessage = messages[-1].content\n",
    "    question = messages[0].content\n",
    "    template = \"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\"\n",
    "    prompt= PromptTemplate(\n",
    "        template= template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    chain = prompt|az_structured_model\n",
    "    scored_result=chain.invoke({\"question\": question, \"context\": lastmessage})\n",
    "    score=scored_result.binary_score     \n",
    "    if score==\"yes\":\n",
    "        print(\"----DECISION: DOCS ARE RELEVANT----\")\n",
    "        return \"generator\"\n",
    "    else:\n",
    "        print(\"----DECISION: DOCS ARE NOT RELEVANT----\")\n",
    "        return \"rewriter\"\n",
    "\n",
    "\n",
    "def rewrite(state:AgentState) -> AgentState:\n",
    "    print(\"---- Rewrite Call----\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    prompt = PromptTemplate(\n",
    "        template= \"\"\"\n",
    "                Look at the input and try to reason about the underlying semantic intent or meaning. \n",
    "                    Here is the initial question: {question} \n",
    "                    Formulate an improved question:\n",
    "            \"\"\",\n",
    "            input_variables=[\"question\"]\n",
    "                    )\n",
    "    chain = prompt | az_chat_model\n",
    "    response = chain.invoke({\"question\" : question})\n",
    "    print(response.content)\n",
    "    return {\"messages\":[response]} \n",
    "\n",
    "\n",
    "def generate(state: AgentState)-> AgentState:\n",
    "    \"\"\" This  \"\"\"\n",
    "    print(\"---- Generate Call----\")\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain = rag_prompt | az_chat_model\n",
    "    response = chain.invoke({\"context\": context, \"question\": question})\n",
    "    return {\"messages\":[response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c56bbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x00000273F37E4B90>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "workflow.add_node(\"LLM Decision Maker\",llm_decision_maker)\n",
    "workflow.add_node(\"Vector Retriever\",ToolNode(tools=tools))\n",
    "workflow.add_node(\"Output Generator\",generate)\n",
    "workflow.add_node(\"Query Rewrite\",rewrite)\n",
    "\n",
    "\n",
    "workflow.add_edge(START,\"LLM Decision Maker\")\n",
    "workflow.add_conditional_edges(\"LLM Decision Maker\",tools_condition,\n",
    "                                 {\n",
    "                                     \"tools\" : \"Vector Retriever\",\n",
    "                                     END:END\n",
    "                                 }                    \n",
    "                              )\n",
    "\n",
    "workflow.add_conditional_edges(\"Vector Retriever\",grade_documents,\n",
    "                                 {\n",
    "                                     \"generator\" : \"Output Generator\",\n",
    "                                     \"rewriter\" : \"Query Rewrite\"\n",
    "                                 }                    \n",
    "                              )\n",
    "\n",
    "workflow.add_edge(\"Output Generator\",END)\n",
    "workflow.add_edge(\"Query Rewrite\",\"LLM Decision Maker\")\n",
    "compiledflow=workflow.compile()\n",
    "compiledflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e7e72af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Decision Maker Call.....\n",
      "Grade Called.....\n",
      "----DECISION: DOCS ARE RELEVANT----\n",
      "---- Generate Call----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LLM Powered Autonomous Agent and Explain the planning and reflection', additional_kwargs={}, response_metadata={}, id='a8830ca8-8ed2-4355-993e-34f5fac3b321'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_w8tiTMKbgZ2TN6IoKWtqSDIU', 'function': {'arguments': '{\"query\":\"LLM Powered Autonomous Agent planning and reflection\"}', 'name': 'Blog-retriver'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 71, 'total_tokens': 96, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_57db37749c', 'id': 'chatcmpl-BklQYc9A9vSdw3A3gtl4noRdBPfNi', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--6d591e65-0e86-4136-b824-46d854b813dd-0', tool_calls=[{'name': 'Blog-retriver', 'args': {'query': 'LLM Powered Autonomous Agent planning and reflection'}, 'id': 'call_w8tiTMKbgZ2TN6IoKWtqSDIU', 'type': 'tool_call'}], usage_metadata={'input_tokens': 71, 'output_tokens': 25, 'total_tokens': 96, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)', name='Blog-retriver', id='ef908900-7bb3-46c5-b114-04adde8eb2cc', tool_call_id='call_w8tiTMKbgZ2TN6IoKWtqSDIU'),\n",
       "  AIMessage(content='An LLM Powered Autonomous Agent utilizes a large language model as its core controller to perform tasks by planning and reflecting on its actions. Planning involves breaking down complex tasks into manageable subgoals and utilizing techniques like Chain of Thought and Tree of Thoughts for effective problem-solving. Reflection allows the agent to learn from past actions, refine its approach, and improve future performance through self-criticism and iterative learning.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 2922, 'total_tokens': 3003, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_57db37749c', 'id': 'chatcmpl-BklQc6YFgNbcKWOSz7HrQMxlEPdFc', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--992238fc-5ac2-4471-b1be-424059321eec-0', usage_metadata={'input_tokens': 2922, 'output_tokens': 81, 'total_tokens': 3003, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiledflow.invoke({\"messages\":\"What is LLM Powered Autonomous Agent and Explain the planning and reflection\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f12465b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Decision Maker Call.....\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Hi How are you?', additional_kwargs={}, response_metadata={}, id='66a55966-1aba-4899-bb05-81b12ed5e4c9'),\n",
       "  AIMessage(content=\"I'm just a computer program, so I don't have feelings, but I'm here and ready to help you! How can I assist you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 63, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_7a53abb7a2', 'id': 'chatcmpl-BklQdC8sHdx01flPfasKdmuXSp97U', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--5731c95d-f71b-4e3d-bce9-6e0eff75651f-0', usage_metadata={'input_tokens': 63, 'output_tokens': 30, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiledflow.invoke({\"messages\":\"Hi How are you?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "114a9d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Decision Maker Call.....\n",
      "Grade Called.....\n",
      "----DECISION: DOCS ARE RELEVANT----\n",
      "---- Generate Call----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How Chain of Hindsight help in Self-Reflection agent systems?', additional_kwargs={}, response_metadata={}, id='dc973b26-266d-4402-bab3-79773c987ed5'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4618oYEbgSSfvExr113M72Aw', 'function': {'arguments': '{\"query\":\"Chain of Hindsight Self-Reflection agent systems\"}', 'name': 'Blog-retriver'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 71, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_57db37749c', 'id': 'chatcmpl-BklWdgWiOhREs1XBWZ1GcLuu4i0XO', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--4c01e2fe-6f6a-4076-8996-5693a42c701b-0', tool_calls=[{'name': 'Blog-retriver', 'args': {'query': 'Chain of Hindsight Self-Reflection agent systems'}, 'id': 'call_4618oYEbgSSfvExr113M72Aw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 71, 'output_tokens': 26, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\n\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equip agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\n\\nIllustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\n\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\n\\nExperiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\n\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\n\\nAfter fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\n\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\n\\nIllustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\n\\n}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\nLLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\n\\nThe generative agent architecture. (Image source: Park et al. 2023)', name='Blog-retriver', id='c1af1bbc-a321-41e8-bb25-f189bc4f044e', tool_call_id='call_4618oYEbgSSfvExr113M72Aw'),\n",
       "  AIMessage(content='Chain of Hindsight (CoH) enhances self-reflection in agent systems by providing a structured way for models to learn from their past outputs through feedback. By presenting a sequence of previous outputs along with human-provided feedback, the model can self-reflect and improve its future responses based on this historical context. This iterative learning process allows the model to produce increasingly refined outputs over time.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 2752, 'total_tokens': 2830, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_57db37749c', 'id': 'chatcmpl-BklWhNoP71HAxxp1j1DkkRG3JUgWI', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run--2ffbb190-122a-4818-b5b2-f632295ba3c7-0', usage_metadata={'input_tokens': 2752, 'output_tokens': 78, 'total_tokens': 2830, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compiledflow.invoke({\"messages\":\"How Chain of Hindsight help in Self-Reflection agent systems?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
